{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61fe113c-10e0-4614-83b7-261bd3b9067d",
   "metadata": {},
   "source": [
    "<p  style=\"font-size:30px; text-align:center; font-weight:bold\">T5 (Text-to-Text Transfer Transformer)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f217e302-f279-4740-aff5-aac9c058a033",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c906f52-5524-4453-ae6e-bcd1f15921cd",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">The T5 model, which was developed by Google Research, is based on the idea that practically all NLP problems can be reduced to text-to-text issues. In other words, the input and output can both be treated as text for any given purpose, be it translation, summarization, question answering, or another. The model is uniformly trained to translate textual inputs into textual outputs.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafbfd3e-eb9d-4f91-92ae-71e04390d1fd",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px; font-weight:bold\"><u>Justification (Why used for medical chatbot (doctor-patient dialogues dataset))</u></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df5d6f9-d0ab-4f9e-b3b4-4d192d4e5437",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">T5, with its text-to-text design, offers a streamlined approach suitable for building medical chatbots using doctor-patient dialogues. It's pre-trained on vast texts and can be further refined for medical conversations, making it both versatile and effective. Whether it's giving brief advice or going into detailed topics, T5 is adept due to its proven performance and deep contextual understanding, important for sensitive medical conversations.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4337a0-6d5a-4122-beb5-2b0da0d6457f",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px; font-weight:bold\"><u>Version</u></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7b554c-e4d4-4159-8bef-d54537aaf717",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">T5-small, version has been used in this project for the training on patient-doctor dialogues. T5-small has approximately 60 million parameters.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00af172a-e5e5-4a31-bb21-75bd07746c5d",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252eb655-ff24-4ecb-93dc-b370f0380478",
   "metadata": {},
   "source": [
    "<p id=\"lib\" style=\"font-size:30px; text-align:center; font-weight:bold\">Required libraries or packages</p> <a href=\"#top\">Back To Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b960a6-f7b1-43ad-9526-1fbcbc0f8e1b",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "615130d3-a7e4-47fa-9fea-fdfb84a62f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-06 03:15:50.274225: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-06 03:15:50.778040: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2023-09-06 03:15:50.778082: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2023-09-06 03:15:50.778087: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import torch # PyTorch open-source deep learning framework\n",
    "import json # json library to load JSON data\n",
    "import pandas as pd # for dataframe\n",
    "from sklearn.model_selection import train_test_split # train_test_split method from scikit learn for datset splitting \n",
    "from transformers import (\n",
    "    T5Tokenizer, # T5 tokenizer\n",
    "    DataCollatorForLanguageModeling,  # collates batches for language modeling tasks\n",
    "    T5ForConditionalGeneration, # T5 model\n",
    "    TrainingArguments, # class to store arguments for training transformers models\n",
    "    Trainer,  # trainer class for training and evaluating transformers models\n",
    "    TrainerCallback, # callbacks used with the trainer class\n",
    "    IntervalStrategy, # enumeration of different interval strategies for logging\n",
    "  \n",
    ")\n",
    "from datasets import load_metric # load metric utility from the datasets library\n",
    "from torch.nn import CrossEntropyLoss # method from pytorch to calculate loss\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader # pytorch utility that provides iterators over datasets, which helps in efficient data feeding\n",
    "from nltk.translate.bleu_score import corpus_bleu # bleu_score from nltk to calculate BLEU score (evaluation metric)\n",
    "from rouge import Rouge # rouge to calculate ROUGE score (evaluation metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778cf44b-8b38-4fe0-a8f1-53d92059110b",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc300285-a2f9-4224-becd-8cd7d15e7d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # check if GPU support available than use GPU otherwise use CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9c4688-96f3-42ab-8734-dbf809cc3a72",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 23px; margin-top: 10px; font-weight: bold\">Load Dataset</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd93ff0d-0bd0-4cc4-baf1-20660d32c382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61776\n"
     ]
    }
   ],
   "source": [
    "with open(\"cleaned_medical_dialogues_dataset.json\", \"r\") as f: # load the data from the JSON file\n",
    "    data = json.load(f)\n",
    "\n",
    "data = data[::4] # sampling 1/4th of the dataset to manage time and computational resources\n",
    "\n",
    "print(len(data)) # print length number of samples in the sampled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5bb73a-cfbb-4b30-bee5-e20fcc276834",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c49051e-875d-4c9a-8504-bc572215332b",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 23px; margin-top: 10px; font-weight: bold\">Data Formatting</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dca07fa-d6cf-4e98-8373-79c4adba49ce",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">\n",
    "The T5 model treats every NLP task as converting one text into another. In the context of doctor-patient dialogues, combine the description and patient's question to give the model a full picture (source), and the doctor's response is the desired answer (target). This setup aligns with T5's design, allowing it to use the given context to produce an appropriate doctor's response.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b70a9ba9-7ffd-4dce-adfe-925810b626e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [] # list for saving queries\n",
    "targets = [] # list for saving responses\n",
    "\n",
    "for item in data: # loop to save dialogues in sources and targets list \n",
    "    source = item[\"Description\"] + \" \" + item[\"Patient\"]\n",
    "    target = item[\"Doctor\"]\n",
    "    sources.append(source)\n",
    "    targets.append(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de35b458-f2ea-41cf-a8f0-7bd907fd1b25",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2e6417-a6b7-4f45-bf3a-65ea51a834bb",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 23px; margin-top: 10px; font-weight: bold\">Prepare the Dataset </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bacc8dfb-d3f2-43bf-a85e-3809ab33b927",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\") # tokenization method of t5 pre-trained model from transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "549bfe5e-d659-4dbd-9157-a6881b0d8034",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoctorPatientDataset(Dataset):\n",
    "    def __init__(self, sources, targets, tokenizer, max_length=512): # initialization method for the dataset\n",
    "        self.sources = sources # list of source sequences\n",
    "        self.targets = targets # list of target sequences\n",
    "        self.tokenizer = tokenizer  # T5 tokenizer for tokenizing input data\n",
    "        self.max_length = max_length  # maximum token length for each sequence\n",
    "        \n",
    "    def __len__(self): # returns the number of items in the dataset\n",
    "        return len(self.sources)\n",
    "    \n",
    "    # fetches and returns the source-target pair at the given index 'idx'\n",
    "    def __getitem__(self, idx):\n",
    "        source = self.sources[idx] # get the source sequence at the specified index\n",
    "        target = self.targets[idx] # get the corresponding target sequence\n",
    "        \n",
    "        # tokenize and format the source sequence, truncatation and padding to 'max_length'.\n",
    "        source_encodings = tokenizer(source, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
    "        \n",
    "        # tokenize and format the target sequence\n",
    "        target_encodings = tokenizer(target, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
    "        \n",
    "        # prepare the data in the format T5 model expects with necessary fields like input_ids, attention_mask, etc.\n",
    "        item = {\n",
    "            \"input_ids\": source_encodings[\"input_ids\"].squeeze(), # source sequence input IDs\n",
    "            \"attention_mask\": source_encodings[\"attention_mask\"].squeeze(), # source sequence attention masks\n",
    "            \"decoder_input_ids\": target_encodings[\"input_ids\"].squeeze(), # target sequence input IDs for the decoder\n",
    "            \"decoder_attention_mask\": target_encodings[\"attention_mask\"].squeeze(), # target sequence attention masks for the decoder\n",
    "            \"labels\": target_encodings[\"input_ids\"].squeeze()  # labels for calculating loss during training\n",
    "        }\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e57bf21f-ee4b-4c24-9229-ae94cd698859",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DoctorPatientDataset(sources, targets, tokenizer) # calling the DoctorPatientDataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d51be8bd-28b6-430e-b7d7-6229018c9c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling( \n",
    "    tokenizer=tokenizer, # assigning the tokenizer we have intialized earlier\n",
    "    mlm=False # passing false to mention it is not masked langauge modelling (like BERT) task \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5b1aa8-a6ed-4a0f-9ba3-bcc2e079ca08",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52755ce-9fbf-46ed-9efc-1825e06aa900",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 23px; margin-top: 10px; font-weight: bold\">Split Dataset</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b995eafe-ab42-4ca4-9087-d8549bc3ad70",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">\n",
    "  The training set and validation/test set are two sets that are typically separated from the available dataset in machine learning and deep learning.\n",
    "This separation helps in evaluating the model's performance on unseen data. The main goal is to prevent the model from overfitting the training set of data. If a model performs exceptionally well on training data but poorly on validation data, it is obviously overfitted.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02a9dfae-c1c6-4f48-a9cc-8f5bc22ecac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2)  # 20% of data as validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c30825b-31cf-4dc9-844e-253b099bd7dd",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">The parameter <b>test_size=0.2</b> indicates that 20% of the dataset will be used as the validation set, while the remaining 80% will be used for training.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071edb59-56b4-41ed-974f-cb06128770cd",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23636201-3be4-41e2-9faa-fe54b711e27d",
   "metadata": {},
   "source": [
    "<p id=\"lib\" style=\"font-size:30px; text-align:center; font-weight:bold\">Fine tuning T5-small</p> <a href=\"#top\">Back To Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5212ba7a-cb26-4c47-bb88-56148a1b2c46",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5bd9f5-fd3b-4218-8db2-56352779e6ae",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 23px; margin-top: 10px; font-weight: bold\">Define Early Stopping</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45e5cc1-f605-4472-b378-4936eddeeb91",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">Callback function to halt the training process when there is no improvement in the model\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "182dbbca-7ff1-4dd7-98b0-9b6bf10667b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingCallback(TrainerCallback):\n",
    "    def __init__(self, early_stopping_patience): # intialize the callback with number of epochs to wait\n",
    "        self.early_stopping_patience = early_stopping_patience\n",
    "        self.patience_counter = 0 # counter to keep track of epochs\n",
    "        self.best_score = None # store the best evaluation score\n",
    "\n",
    "    def on_evaluate(self, args, state, control, metrics, **kwargs):\n",
    "        current_score = metrics.get(\"eval_loss\") # monitoring 'eval_loss'\n",
    "\n",
    "        if self.best_score is None or current_score < self.best_score: #compare the evaluation score and reset the patience counter\n",
    "            self.best_score = current_score\n",
    "            self.patience_counter = 0\n",
    "        else:\n",
    "            self.patience_counter += 1\n",
    "\n",
    "        if self.patience_counter >= self.early_stopping_patience: # condition if the patient counter exceeds or equal than stop training \n",
    "            control.should_training_stop = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1588c4a3-7a0c-4a1b-8509-484856a1a3c7",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd84ef3-b6a4-448f-b2ec-ac64a34d9fb7",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 23px; margin-top: 10px; font-weight: bold\">Model Intialization and Define Training Argument</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e7562d5-6ed6-49d7-8071-6e71923a61ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device) # model Initialization\n",
    "\n",
    "# setting the training arguments or parameters for training of the model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./medical_t5_finetuned', # saved model directory\n",
    "    num_train_epochs=2, # number of epochs 2, tried with more than 2, but it was taking a lot of time so for the prototype just limited to 2 \n",
    "    per_device_train_batch_size=2, # batch size 2\n",
    "    logging_dir='./t5_logs', # saving logs directory\n",
    "    logging_steps=10, # logging after 10 steps to check losses\n",
    "    save_steps=10000, # saving the model after 10000 steps \n",
    "    save_strategy=\"epoch\",  # save model after each epoch\n",
    "    evaluation_strategy=\"epoch\",  # evaluate after each epoch with validation dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f411723-7885-4ceb-b9a3-635747913d96",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cde07a-cf2c-4c35-bd54-39ee2ba521cf",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 23px; margin-top: 10px; font-weight: bold\">Initialize  Trainer</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e447d61c-48b9-41a0-b5e0-acb11cef8812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the Trainer instance \n",
    "trainer = Trainer(\n",
    "    model=model, # passing the model to be fine tuned\n",
    "    args=training_args, # passing the training configurations\n",
    "    data_collator=data_collator, # data batching and format\n",
    "    train_dataset=train_dataset, # passing the training dataset\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)], # callback function for ealry stopping, with 2 epoch\n",
    "    eval_dataset=val_dataset # passing the validation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8bb09b-474f-414c-99ee-599921abac84",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20022df-8513-40fb-a46d-4ec6c22df83b",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 23px; margin-top: 10px; font-weight: bold\">Train Model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43a14d79-d296-4c57-97d2-ea8f91de42d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msc1/anaconda3/envs/Env-7145COMP/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 49420\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 49420\n",
      "  Number of trainable parameters = 60506624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='49420' max='49420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [49420/49420 17:48:45, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.514900</td>\n",
       "      <td>2.897026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.320200</td>\n",
       "      <td>2.535566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 12356\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./medical_t5_finetuned/checkpoint-24710\n",
      "Configuration saved in ./medical_t5_finetuned/checkpoint-24710/config.json\n",
      "Model weights saved in ./medical_t5_finetuned/checkpoint-24710/pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 12356\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to ./medical_t5_finetuned/checkpoint-49420\n",
      "Configuration saved in ./medical_t5_finetuned/checkpoint-49420/config.json\n",
      "Model weights saved in ./medical_t5_finetuned/checkpoint-49420/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=49420, training_loss=3.7003659997088665, metrics={'train_runtime': 64126.876, 'train_samples_per_second': 1.541, 'train_steps_per_second': 0.771, 'total_flos': 1.337718365749248e+16, 'train_loss': 3.7003659997088665, 'epoch': 2.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() # training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b07aeb-de35-453c-a6e7-09f0a4901c8c",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf8953e-bab3-4fad-97c9-f41a4e29e4ad",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 23px; margin-top: 10px; font-weight: bold\">Save the model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dc39466-0580-402b-9d1b-3c08643f902b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./medical_t5_finetuned\n",
      "Configuration saved in ./medical_t5_finetuned/config.json\n",
      "Model weights saved in ./medical_t5_finetuned/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(\"./medical_t5_finetuned\") # save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33711e6-16de-45bb-99dc-43d74272aa1f",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cd674b-19a9-4a32-b546-aa2ad54a8700",
   "metadata": {},
   "source": [
    "<p ><center><u style=\"font-size: 28px; margin-top: 10px; font-weight: bold\">Model Evaluation</u></center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99b3955-3b91-4f17-81a1-030d3d52a765",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">\n",
    "Model evaluation is an important part of creating machine learning models. It's about testing how good the model is using data it hasn't seen during training, usually called test or validation data. We do this to see if the model's answers are right and understand any mistakes it might make.\n",
    "</p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f03ac8a-65df-48ae-95c3-5119b0333904",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210b3c6a-aff2-47b2-9d78-e7545882c5ef",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 23px; margin-top: 10px; font-weight: bold\">Load the model</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1de33940-d24f-42d7-b38d-3f7b4b1eccb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./medical_t5_finetuned/config.json\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file ./medical_t5_finetuned/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at ./medical_t5_finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "finetuned_model = T5ForConditionalGeneration.from_pretrained(\"./medical_t5_finetuned\")\n",
    "finetuned_model = finetuned_model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f12892e-2fcf-4d50-bd5b-5e0d116c6f72",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5432835d-9d70-4f38-8d21-3f50c2dac066",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 23px; margin-top: 10px; font-weight: bold\">Loss and Perplexity</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ca5eba-55e7-42b3-8e96-24340235a87e",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">\n",
    "Loss measures the difference between a model's predictions and the actual data. It helps in adjusting the model to make better predictions. By looking at the loss, we can see if the model is improving and make necessary changes if needed.\n",
    "</p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9c7eea-19a5-44dc-8d31-18f2dc369d91",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">\n",
    "    Perplexity checks how good a model is at guessing the next word. For medical chatbots, a lower value means the bot can chat more smoothly and make sense.\n",
    "</p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d0cd4c-3672-427e-881e-dfb138bd7d98",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px; margin-top: 10px; font-weight: bold\">ROUGE score</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e46bc32-1af1-4fc4-8249-bcebd57d5fff",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">\n",
    "    The ROUGE score looks at how much the predicted text matches the reference text using different measures like precision, recall, and F1-score. It's particularly useful for tasks like summarization to see how much key information the model includes in its output. In the context of medical chatbot, ROUGE can help determine how closely the generated response matches a desired or reference answer, indicating the system's ability to provide accurate and relevant information.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "862c3f62-2b2d-4d4f-bfb9-3dfcc9538388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msc1/anaconda3/envs/Env-7145COMP/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 22.296912651062012, 'perplexity': 4824211456.0, 'accuracy': 0.004140625, 'rouge-1': {'r': 0.0189294489628268, 'p': 0.04288250505494566, 'f': 0.023984178275725195}, 'rouge-2': {'r': 0.0005700558142582217, 'p': 0.0011365764373910494, 'f': 0.0006913916670250918}, 'rouge-l': {'r': 0.011724279000924332, 'p': 0.026785558832261913, 'f': 0.014857218795111458}}\n"
     ]
    }
   ],
   "source": [
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "def compute_metrics_on_subset(model, subset, tokenizer): # function definition to calculate evaluation metrics\n",
    "    # evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # initialize the variables for metrics storing\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "\n",
    "    # lists to collect all predicted and actual sentences for ROUGE score\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "\n",
    "    # loss criterion initialization\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for item in subset:\n",
    "            # extract tensors and move them to the correct device\n",
    "            input_ids = item['input_ids'].unsqueeze(0).to(device)\n",
    "            attention_mask = item['attention_mask'].unsqueeze(0).to(device)\n",
    "            decoder_input_ids = item['decoder_input_ids'].unsqueeze(0).to(device)\n",
    "            labels = item['labels'].unsqueeze(0).to(device)\n",
    "\n",
    "            # model forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=decoder_input_ids, labels=labels)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "            # calculate loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # calculate accuracy\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_count += labels.numel()\n",
    "\n",
    "            # decode for ROUGE score\n",
    "            predictions.append(tokenizer.decode(preds.squeeze()))\n",
    "            actuals.append(tokenizer.decode(labels.squeeze()))\n",
    "\n",
    "    # calculate perplexity from the average loss\n",
    "    perplexity = torch.exp(torch.tensor(total_loss / len(subset)))\n",
    "\n",
    "    # calculate accuracy\n",
    "    accuracy = total_correct / total_count\n",
    "\n",
    "    # calculate ROUGE scores\n",
    "    rouge_scores = rouge.compute(predictions=predictions, references=actuals)\n",
    "\n",
    "    rouge1 = {\n",
    "        'r': rouge_scores['rouge1'].mid.recall,\n",
    "        'p': rouge_scores['rouge1'].mid.precision,\n",
    "        'f': rouge_scores['rouge1'].mid.fmeasure\n",
    "    }\n",
    "    rouge2 = {\n",
    "        'r': rouge_scores['rouge2'].mid.recall,\n",
    "        'p': rouge_scores['rouge2'].mid.precision,\n",
    "        'f': rouge_scores['rouge2'].mid.fmeasure\n",
    "    }\n",
    "    rougel = {\n",
    "        'r': rouge_scores['rougeL'].mid.recall,\n",
    "        'p': rouge_scores['rougeL'].mid.precision,\n",
    "        'f': rouge_scores['rougeL'].mid.fmeasure\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / len(subset),\n",
    "        \"perplexity\": perplexity.item(),\n",
    "        \"accuracy\": accuracy,\n",
    "        \"rouge-1\": rouge1,\n",
    "        \"rouge-2\": rouge2,\n",
    "        \"rouge-l\": rougel\n",
    "    }\n",
    "\n",
    "subset_length = 100  # subset from validation dataset\n",
    "subset = [val_dataset[i] for i in range(subset_length)]\n",
    "metrics = compute_metrics_on_subset(finetuned_model, subset, tokenizer)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e025274-1169-4d32-a8ea-ee3939115960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 22.296912651062012,\n",
       " 'perplexity': 4824211456.0,\n",
       " 'accuracy': 0.004140625,\n",
       " 'rouge-1': {'r': 0.0189294489628268,\n",
       "  'p': 0.04288250505494566,\n",
       "  'f': 0.023984178275725195},\n",
       " 'rouge-2': {'r': 0.0005700558142582217,\n",
       "  'p': 0.0011365764373910494,\n",
       "  'f': 0.0006913916670250918},\n",
       " 'rouge-l': {'r': 0.011724279000924332,\n",
       "  'p': 0.026785558832261913,\n",
       "  'f': 0.014857218795111458}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690ec4f8-de74-4a2f-99b1-a7d3a78efc89",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 23px; margin-top: 10px; font-weight: bold\"><u>Saving Results to the dataframe</u></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebac1ea-3b56-4592-ba15-83a3ccf7ccd3",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">\n",
    "  I am saving the results in the dataframe one by one of each model so i can compare the results in the separate python file (medical_chatbot_eval_metrics.ipynb).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "709db85b-ac53-454e-bf9e-abdccc4ba1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics_results = {\n",
    "    'model_name': 'T5-small',\n",
    "    'loss': metrics['loss'],\n",
    "    'perplexity': metrics['perplexity'],\n",
    "    'accuracy': metrics['accuracy'],\n",
    "    'rouge-1_r': metrics['rouge-1']['r'],\n",
    "    'rouge-1_p': metrics['rouge-1']['p'],\n",
    "    'rouge-1_f': metrics['rouge-1']['f'],\n",
    "    'rouge-2_r': metrics['rouge-2']['r'],\n",
    "    'rouge-2_p': metrics['rouge-2']['p'],\n",
    "    'rouge-2_f': metrics['rouge-2']['f'],\n",
    "    'rouge-l_r': metrics['rouge-l']['r'],\n",
    "    'rouge-l_p': metrics['rouge-l']['p'],\n",
    "    'rouge-l_f': metrics['rouge-l']['f']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e85a8f31-4a68-4050-b69b-1e64d644237b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'T5-small',\n",
       " 'loss': 22.296912651062012,\n",
       " 'perplexity': 4824211456.0,\n",
       " 'accuracy': 0.004140625,\n",
       " 'rouge-1_r': 0.0189294489628268,\n",
       " 'rouge-1_p': 0.04288250505494566,\n",
       " 'rouge-1_f': 0.023984178275725195,\n",
       " 'rouge-2_r': 0.0005700558142582217,\n",
       " 'rouge-2_p': 0.0011365764373910494,\n",
       " 'rouge-2_f': 0.0006913916670250918,\n",
       " 'rouge-l_r': 0.011724279000924332,\n",
       " 'rouge-l_p': 0.026785558832261913,\n",
       " 'rouge-l_f': 0.014857218795111458}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91c47fde-2e46-42e4-9b5d-351ba5850843",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>loss</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>rouge-1_r</th>\n",
       "      <th>rouge-1_p</th>\n",
       "      <th>rouge-1_f</th>\n",
       "      <th>rouge-2_r</th>\n",
       "      <th>rouge-2_p</th>\n",
       "      <th>rouge-2_f</th>\n",
       "      <th>rouge-l_r</th>\n",
       "      <th>rouge-l_p</th>\n",
       "      <th>rouge-l_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Encoder-Decoder LSTM</td>\n",
       "      <td>0.074382</td>\n",
       "      <td>1.052910</td>\n",
       "      <td>0.990507</td>\n",
       "      <td>0.985153</td>\n",
       "      <td>0.967791</td>\n",
       "      <td>0.976377</td>\n",
       "      <td>0.975947</td>\n",
       "      <td>0.946462</td>\n",
       "      <td>0.960935</td>\n",
       "      <td>0.985153</td>\n",
       "      <td>0.967791</td>\n",
       "      <td>0.976377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-2 Medium</td>\n",
       "      <td>10.379869</td>\n",
       "      <td>32204.726562</td>\n",
       "      <td>0.009531</td>\n",
       "      <td>0.497472</td>\n",
       "      <td>0.526547</td>\n",
       "      <td>0.511501</td>\n",
       "      <td>0.176954</td>\n",
       "      <td>0.186899</td>\n",
       "      <td>0.181735</td>\n",
       "      <td>0.388866</td>\n",
       "      <td>0.411154</td>\n",
       "      <td>0.399379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Facebook/BART-base</td>\n",
       "      <td>0.393779</td>\n",
       "      <td>1.482573</td>\n",
       "      <td>0.920898</td>\n",
       "      <td>0.946877</td>\n",
       "      <td>0.954811</td>\n",
       "      <td>0.950963</td>\n",
       "      <td>0.911431</td>\n",
       "      <td>0.918899</td>\n",
       "      <td>0.915209</td>\n",
       "      <td>0.933774</td>\n",
       "      <td>0.941522</td>\n",
       "      <td>0.937755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model_name       loss    perplexity  accuracy  rouge-1_r  \\\n",
       "0  Encoder-Decoder LSTM   0.074382      1.052910  0.990507   0.985153   \n",
       "1          GPT-2 Medium  10.379869  32204.726562  0.009531   0.497472   \n",
       "2    Facebook/BART-base   0.393779      1.482573  0.920898   0.946877   \n",
       "\n",
       "   rouge-1_p  rouge-1_f  rouge-2_r  rouge-2_p  rouge-2_f  rouge-l_r  \\\n",
       "0   0.967791   0.976377   0.975947   0.946462   0.960935   0.985153   \n",
       "1   0.526547   0.511501   0.176954   0.186899   0.181735   0.388866   \n",
       "2   0.954811   0.950963   0.911431   0.918899   0.915209   0.933774   \n",
       "\n",
       "   rouge-l_p  rouge-l_f  \n",
       "0   0.967791   0.976377  \n",
       "1   0.411154   0.399379  \n",
       "2   0.941522   0.937755  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics_results_dataframe = pd.read_csv('eval_metrics_results_dataframe.csv') # load the csv  \n",
    "eval_metrics_results_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71f2c7f6-6306-4ce7-8e0c-9ec309f2995a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>loss</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>rouge-1_r</th>\n",
       "      <th>rouge-1_p</th>\n",
       "      <th>rouge-1_f</th>\n",
       "      <th>rouge-2_r</th>\n",
       "      <th>rouge-2_p</th>\n",
       "      <th>rouge-2_f</th>\n",
       "      <th>rouge-l_r</th>\n",
       "      <th>rouge-l_p</th>\n",
       "      <th>rouge-l_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Encoder-Decoder LSTM</td>\n",
       "      <td>0.074382</td>\n",
       "      <td>1.052910e+00</td>\n",
       "      <td>0.990507</td>\n",
       "      <td>0.985153</td>\n",
       "      <td>0.967791</td>\n",
       "      <td>0.976377</td>\n",
       "      <td>0.975947</td>\n",
       "      <td>0.946462</td>\n",
       "      <td>0.960935</td>\n",
       "      <td>0.985153</td>\n",
       "      <td>0.967791</td>\n",
       "      <td>0.976377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-2 Medium</td>\n",
       "      <td>10.379869</td>\n",
       "      <td>3.220473e+04</td>\n",
       "      <td>0.009531</td>\n",
       "      <td>0.497472</td>\n",
       "      <td>0.526547</td>\n",
       "      <td>0.511501</td>\n",
       "      <td>0.176954</td>\n",
       "      <td>0.186899</td>\n",
       "      <td>0.181735</td>\n",
       "      <td>0.388866</td>\n",
       "      <td>0.411154</td>\n",
       "      <td>0.399379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Facebook/BART-base</td>\n",
       "      <td>0.393779</td>\n",
       "      <td>1.482573e+00</td>\n",
       "      <td>0.920898</td>\n",
       "      <td>0.946877</td>\n",
       "      <td>0.954811</td>\n",
       "      <td>0.950963</td>\n",
       "      <td>0.911431</td>\n",
       "      <td>0.918899</td>\n",
       "      <td>0.915209</td>\n",
       "      <td>0.933774</td>\n",
       "      <td>0.941522</td>\n",
       "      <td>0.937755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T5-small</td>\n",
       "      <td>22.296913</td>\n",
       "      <td>4.824211e+09</td>\n",
       "      <td>0.004141</td>\n",
       "      <td>0.018929</td>\n",
       "      <td>0.042883</td>\n",
       "      <td>0.023984</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.011724</td>\n",
       "      <td>0.026786</td>\n",
       "      <td>0.014857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model_name       loss    perplexity  accuracy  rouge-1_r  \\\n",
       "0  Encoder-Decoder LSTM   0.074382  1.052910e+00  0.990507   0.985153   \n",
       "1          GPT-2 Medium  10.379869  3.220473e+04  0.009531   0.497472   \n",
       "2    Facebook/BART-base   0.393779  1.482573e+00  0.920898   0.946877   \n",
       "3              T5-small  22.296913  4.824211e+09  0.004141   0.018929   \n",
       "\n",
       "   rouge-1_p  rouge-1_f  rouge-2_r  rouge-2_p  rouge-2_f  rouge-l_r  \\\n",
       "0   0.967791   0.976377   0.975947   0.946462   0.960935   0.985153   \n",
       "1   0.526547   0.511501   0.176954   0.186899   0.181735   0.388866   \n",
       "2   0.954811   0.950963   0.911431   0.918899   0.915209   0.933774   \n",
       "3   0.042883   0.023984   0.000570   0.001137   0.000691   0.011724   \n",
       "\n",
       "   rouge-l_p  rouge-l_f  \n",
       "0   0.967791   0.976377  \n",
       "1   0.411154   0.399379  \n",
       "2   0.941522   0.937755  \n",
       "3   0.026786   0.014857  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics_results_dataframe = eval_metrics_results_dataframe.append(eval_metrics_results, ignore_index=True) # append the record in the dataframe\n",
    "eval_metrics_results_dataframe.to_csv('eval_metrics_results_dataframe.csv', index=False) # save to the same file\n",
    "eval_metrics_results_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fefd2ad-679f-496a-bd0d-06f05139a7e4",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e18dce7-ddcb-4139-8bd1-9ba65ab884bb",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 23px; margin-top: 10px; font-weight: bold\"><u>Answer to user queries by using the model</u></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d861d46d-b637-400e-84eb-37af3457249d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical query: is lp?\n"
     ]
    }
   ],
   "source": [
    "def generate_response(input_text, model, tokenizer, max_length=512):\n",
    "    input_text = \"Medical query: \" + input_text # T5 expects this format for seq2seq tasks\n",
    "    input_tensor = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_tensor, max_length=max_length, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "user_query = \"what is nlp?\"\n",
    "response = generate_response(user_query, finetuned_model, tokenizer)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add8f26c-a9d5-4c4b-a74b-5db578f41940",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
