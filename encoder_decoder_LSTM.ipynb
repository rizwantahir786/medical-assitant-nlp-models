{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4ca6c1c-4c20-41a0-b92d-978f4c23eb49",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p id=\"eda\" style=\"font-size:30px; text-align:center; font-weight:bold\">Encoder-Decoder LSTM (Long Short-Term Memory)</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507fd644-899b-4aa1-9d44-583ce91e5fb2",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8772d0e5-c232-4b68-882a-d5c67411e2c8",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">LSTM (Long Short-Term Memory) is a type of recurrent neural network (RNN), which functions similarly to a brain in identifying patterns in a series of data. Unlike typical RNNs, which can forget items from a long time in the past, LSTMs feature a unique design with three gates (input gate, forget gate, and output gate) that help with memory. This makes LSTMs ideal for tasks such as language translation and comprehending spoken speech.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e43efd5-73bb-476b-af0d-722268d71b1d",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">The Encoder-Decoder LSTM architecture is made up of two primary parts. The encoder processes each item in the input sequence and generates a \"context\" vector that represents the prominent aspects of the sequence. The context vector is then used by the decoder to construct an output sequence, which can be word-by-word or character-by-character in language tasks.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1f5c7f-b972-47cc-8dc1-1c3c03b3b256",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px; font-weight:bold\"><u>Justification (Why used for medical chatbot (doctor-patient dialogues dataset))</u></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d18502a-a026-48af-aaa1-65f0540e7b25",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\"> The Encoder-Decoder LSTM is suitable for medical chatbots because it can handle both short and comprehensive doctor-patient conversations. It is effective in recalling the context of health conversations because of its design, which guarantees that it remembers important details. Its versatility makes it appropriate for comprehensive medical conversations and can be utilised for a variety of jobs. It's able to provide appropriate responses even with limited data. In addition, it can be modified to bring attention to important sections in dialogues, which is particularly helpful when some medical terminology or phrases are more crucial than others.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0342f9a5-ec62-4eac-8350-709907acc320",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:3px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a12132-c961-4d78-98b3-c95ee1e2f3d6",
   "metadata": {},
   "source": [
    "<p id=\"lib\" style=\"font-size:30px; text-align:center; font-weight:bold\">Required libraries or packages</p> <a href=\"#top\">Back To Top</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea9ad27-28dd-4331-b451-08be232f36ea",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:3px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2ba85a7-00ae-42d2-8d0b-dc5bcc99f9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-06 02:11:15.412773: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-06 02:11:16.082090: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2023-09-06 02:11:16.082249: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.2/lib64\n",
      "2023-09-06 02:11:16.082254: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # import dataframe library\n",
    "import numpy as np # numpy library for mathematical operations\n",
    "import json # json library to load JSON data\n",
    "from sklearn.model_selection import train_test_split # train_test_split method from scikit learn for datset splitting \n",
    "import tensorflow as tf # tensorflow for building deepl learning models\n",
    "from tensorflow.keras.models import Model # model class from keras\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Layer, Concatenate # different layers and lstm model from keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping # EarlyStopping callback function from keras to stop training when a model has stopped improving\n",
    "from nltk.translate.bleu_score import sentence_bleu # BLEU  metric from NLTK library\n",
    "from rouge import Rouge # another metric ROUGE from rouge library\n",
    "from tensorflow.keras.models import load_model # to load the saved model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dccf94-4d95-4a52-97e6-aac1e794516c",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117b366e-e2c2-4b8f-bb09-251184a85a18",
   "metadata": {},
   "source": [
    "<p ><center><u style=\"font-size: 28px; margin-top: 10px; font-weight: bold\">Pre-processing</u></center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d122e9bd-df3c-4808-839a-34fa9e756759",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">It's crucial to convert the textual input into numerical form that the deep learning model can understand prior to input the data into the model. Tokenization and padding are the steps for this conversion.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12f9e01-caf1-4925-8780-506b5da7ba5f",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 23px; margin-top: 10px; font-weight: bold\">Load Dataset</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "790166c7-67d5-4705-b17a-39a7e5291d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61776\n"
     ]
    }
   ],
   "source": [
    "with open(\"cleaned_medical_dialogues_dataset.json\", \"r\") as f: # load the json content of the file to data variable\n",
    "    data = json.load(f)\n",
    "    \n",
    "data = data[::4] # sampling 1/4th of the dataset to manage time and computational resources\n",
    "\n",
    "print(len(data))  # print length number of samples in the sampled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b709589f-066a-4e05-be2d-499a01e54b6f",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296af599-71aa-45ee-aca0-b80744180934",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 23px; margin-top: 10px; font-weight: bold\">Data Formatting</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0838af5c-4852-48ac-8a3b-d905a1bd79b2",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">The dataset formatting, start and end tokens ahs been added in both patient and doctor dialogues. The start and end tokens help the model recognize the beginning and end of sequences </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bd4ccb6-1e21-4e25-b032-94cce8db3015",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"<start> \" + entry['Patient'] + \" <end>\" for entry in data] # start and end tokens in patient dialogues \n",
    "answers = [\"<start> \" + entry['Doctor'] + \" <end>\" for entry in data] # start and end tokens in doctor dialogues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a15db8-17b5-4883-bb67-438c439ed3dd",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731fd0e0-edd9-4d2f-b8bd-3d58e04a5da6",
   "metadata": {
    "tags": []
   },
   "source": [
    "<p style=\"font-size: 23px; margin-top: 10px; font-weight: bold\">Tokenization</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bbd8e7-e758-496a-a257-d069bab753f1",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">The neural network cannot process the text data so there is a need to convert the text data into numerical data so that it can be processed by the deep learning neural network. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e36f1c8-4766-44c9-8b39-570a63b3b6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer() # initialize tokenizer\n",
    "tokenizer.fit_on_texts(questions + answers) # text fitting in the tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbfb32e3-4d56-4cef-8e67-fa3a1083b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_seq = tokenizer.texts_to_sequences(questions) # conversion of questions into sequences of itegers\n",
    "answers_seq = tokenizer.texts_to_sequences(answers) # conversion of answers into sequences of integers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7433a04-0eba-4bed-ac5f-f1bb9b319c98",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">For medical chatbot dialogue ,based on Seq2Seq models, this phase is especially important since the input (questions) and output (answers) data need to be in a format that the model can understand. Encoder and decoder are the two primary parts of Seq2Seq. The encoder creates a context vector from a query as input. The decoder then generates an answer using this context vector. Both of these require sequences of numbers as their input data.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b362fcab-cd84-46c3-9adf-e76d78a14640",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\"> In this 'questions' are user/patient queries and 'answers' are doctors responses. By converting these dialogues into sequences of integers, the model will be trained to recognise similarities in how doctors answer to particular questions or symptoms and eventually generate similar responses to new questions. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e58b55f-b21c-410c-a338-9599545bcd22",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74282781-5d59-4fd3-acef-6bfb52ca2df5",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 23px; margin-top: 10px; font-weight: bold\">Padding Sequences</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1724c8b4-4703-4965-84a7-5b74825cadd9",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">Padding sequences to have equal length; neural networks require fixed-length input.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a766ea30-057c-46ed-93f2-d807d47c2cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxlen = max([len(seq) for seq in questions_seq + answers_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2aad04a3-1ca8-464f-9b59-d2e352c889fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 256  # truncating sequences to a maximum length for computational efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50bdca30-24a4-4bb0-a901-4d4e1a2e0a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_seq = tf.keras.preprocessing.sequence.pad_sequences(questions_seq, maxlen=maxlen, truncating='post') # using keras method for pad sequence to questions\n",
    "answers_seq = tf.keras.preprocessing.sequence.pad_sequences(answers_seq, maxlen=maxlen , truncating='post') # using keras method for pad sequence to answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd14046-7da0-49bd-904a-c2b5f7a113f6",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">\n",
    "    Neural networks expect input data to have a consistent shape. Sentences and other text sequences might vary in length when you deal with them. For example, \"How are you today?\" consists of four words, whereas \"I am fine.\" has three. A neural network won't function if you feed it directly with sequences of different lengths. The input data is kept in a constant shape, which is required for matrix operations in the neural network, by padding sequences to a given length.\n",
    "Sometimes like in our case of doctor-patient dialogues, truncating is not a good choice as it might truncate contextual information. However, I did it with maxlen of 256 because of limited computational resources</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9382f001-5ee8-4db5-a243-407a34cf8736",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcc317b-3501-446b-adf4-cbafbfbeb956",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 23px; margin-top: 10px; font-weight: bold\">Split Dataset</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48afad38-84a7-4c0a-adc9-7f711acc8d42",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">\n",
    "  The training set and validation/test set are two sets that are typically separated from the available dataset in machine learning and deep learning.\n",
    "This separation helps in evaluating the model's performance on unseen data. The main goal is to prevent the model from overfitting the training set of data. If a model performs exceptionally well on training data but poorly on validation data, it is obviously overfitted.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d66308f0-58d9-4905-9819-c7b987925c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_train, questions_val, answers_train, answers_val = train_test_split(questions_seq, answers_seq, test_size=0.2) # train_test_split function from scikit-learn library to split the datset into train, test and validation sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c66d83-aeae-4e62-bd2b-1f59d55eb44a",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">The parameter <b>test_size=0.2</b> indicates that 20% of the dataset will be used as the validation set, while the remaining 80% will be used for training.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b48ec-3bfc-4d8f-96db-6cc7353359e6",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1c1917-58e4-44bb-819d-df7eca635414",
   "metadata": {},
   "source": [
    "<p ><center><u style=\"font-size: 28px; margin-top: 10px; font-weight: bold\">Model Training</u></center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33526954-48cc-4b72-bd6a-e6b6d3b89490",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee2d275-74de-4a14-bdb5-4249522849a8",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 18px; margin-top: 10px; font-weight: bold\">Constants</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c90ad53-8fe5-467e-8b24-e1fd2dba768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters for training\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 6\n",
    "LSTM_UNITS = 128\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920e96ad-ce20-4efc-af44-4b48c7c36d05",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 18px; margin-top: 10px; font-weight: bold\">BahdanauAttention Class</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e55ed2b-a82f-4771-849c-3e11a001b63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(Layer):\n",
    "    def __init__(self, units, **kwargs): # constructor for the attention layer, three dense layers used to prduce the attention scoe\n",
    "        super(BahdanauAttention, self).__init__(**kwargs)\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, query, values): # method defines the logic for generating attention score \n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values))) # bahdanau' formula to caculatethe attention scores\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "    def get_config(self): # method to support model saving and loading with custom objects\n",
    "        config = super().get_config().copy()\n",
    "        config.update({\n",
    "            'units': self.W1.units\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34d7d9a-03d4-4377-8255-26274ca893a3",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">\n",
    "    The <b>BahdanauAttention</b> class defines an attention mechanism based on the formula introduced by Dzmitry Bahdanau in the context of neural machine translation. This attention mechanism allows the decoder to pay attention to different parts of the input sequence as it decodes, rather than using the encoder's final state alone.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0de3e7d-af7c-48bd-9e37-87ab49708275",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 18px; margin-top: 10px; font-weight: bold\">Answer query function</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3d6dcdf-1b26-4631-b3de-3e27e737c6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query, loaded_model, tokenizer, max_seq_length):\n",
    "    \n",
    "    tokenized_query = tokenizer.texts_to_sequences([query])\n",
    "    padded_query = tf.keras.preprocessing.sequence.pad_sequences(tokenized_query, maxlen=max_seq_length, padding='post')\n",
    "    \n",
    "    decoded_seq = loaded_model.predict([padded_query, padded_query])  \n",
    "    answer_tokens = [np.argmax(token) for token in decoded_seq[0]]\n",
    "    \n",
    "   \n",
    "    if '<start>' not in tokenizer.word_index:  # Check for the end token and truncate the sequence\n",
    "        tokenizer.word_index['<start>'] = max(tokenizer.word_index.values()) + 1\n",
    "\n",
    "    if '<end>' not in tokenizer.word_index:\n",
    "        tokenizer.word_index['<end>'] = max(tokenizer.word_index.values()) + 2\n",
    "\n",
    "    if tokenizer.word_index['<end>'] in answer_tokens:\n",
    "        answer_tokens = answer_tokens[:answer_tokens.index(tokenizer.word_index['<end>'])]\n",
    "\n",
    "    \n",
    "    # convert token IDs back to words, excluding the start and end tokens\n",
    "    answer = ' '.join(tokenizer.index_word[token_id] for token_id in answer_tokens if token_id > 0 and token_id not in [tokenizer.word_index['<start>'], tokenizer.word_index['<end>']])\n",
    "    \n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9ff92a-2da0-4fae-878e-9d3f84f16dfa",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">\n",
    "    The <b>answer_query</b> function has been written to take a user's query, process it, and then generate an answer using a trained sequence to sequence model.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b716bfb0-e0dc-463f-b7db-1763bc3f3a80",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f6d74c-6ff2-4d77-ba61-6ab831a8376d",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 23px; margin-top: 10px; font-weight: bold\"><u>Hyperparameters Tuning</u></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219101f2-2d67-4b3d-ac81-c283ac1d543c",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">\n",
    "    The validation set can be used to test different hyperparameters to determine the best configuration for the doctor-patient dialogues. I chose bayesian optimization hyperparameter technique, which tries to predictively choose the best hyperparameters and faster than random and grid search. However, I tried with small subset of validation data it was taking alot of time. Consequently, due to time constraints, and the computational resources I got, bypassing the hyperparameter tuning step. However, I tried different hyperparameters and combination of it to get the best possible results. In the future work, I will do the comprehensive hyperparameter tuning.\n",
    "</p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26934f7-e8c3-42f3-ac90-890de5df33fc",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 23px; margin-top: 10px; font-weight: bold\"><u>Training</u></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53988d42-da60-4c4a-8195-d7576362f97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(tokenizer.word_index) + 1 # calculate the size of the vocabulary we have in our dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10d4f0f-1606-414f-9a40-98894f384ee6",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px; margin-top: 10px; font-weight: bold\">Encoder</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6a7ca87-d09b-463e-ae4a-2eef0697b198",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-06 02:11:26.236044: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-06 02:11:26.261391: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-06 02:11:26.261860: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-06 02:11:26.262853: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-06 02:11:26.263534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-06 02:11:26.263999: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-06 02:11:26.264465: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-06 02:11:26.809384: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-06 02:11:26.810118: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-06 02:11:26.810569: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-09-06 02:11:26.810994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22265 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = Input(shape=(None,)) # input layer for the encoder\n",
    "encoder_embedding_layer = Embedding(input_dim=VOCAB_SIZE, output_dim=100, mask_zero=True) # embedding layer, the layer which converst integer token indices into dense vectors of fixed size 100\n",
    "encoder_embedding = encoder_embedding_layer(encoder_inputs)\n",
    "\n",
    "encoder_lstm_1 = LSTM(LSTM_UNITS, return_sequences=True, return_state=True, dropout=0.3, recurrent_dropout=0.3) # first lstm layer, returns sequences to use as input for next layer. Dropout 0.3 has been useed to drop neurons for regularization. \n",
    "encoder_output1, state_h1, state_c1 = encoder_lstm_1(encoder_embedding)\n",
    "\n",
    "encoder_lstm_2 = LSTM(LSTM_UNITS, return_sequences=True, return_state=True, dropout=0.3, recurrent_dropout=0.3) #second lstm layer, also return sequences and dropout value has been set\n",
    "encoder_outputs_2, state_h2, state_c2 = encoder_lstm_2(encoder_output1)\n",
    "\n",
    "encoder_states = [state_h1, state_c1, state_h2, state_c2] # LSTM encoder with two layers, used to learn and represent complex relationships in the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3640ed-6455-441e-8b60-fd1b7687a061",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px; margin-top: 10px; font-weight: bold\">Decoder</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a174d4a-e958-4c80-9cd7-594730ceadb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "decoder_inputs = Input(shape=(None,)) # input layer for the decoder\n",
    "decoder_embedding_layer = Embedding(input_dim=VOCAB_SIZE, output_dim=100, mask_zero=True)\n",
    "decoder_embedding = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "attention_layer = BahdanauAttention(LSTM_UNITS) # attention layer, calling the BahdanauAttention class and passing the LSTM Units\n",
    "context_vector, attention_weights = attention_layer(state_h2, encoder_outputs_2)\n",
    "decoder_concat_input = Concatenate(axis=-1)([context_vector, decoder_embedding])\n",
    "\n",
    "decoder_lstm_1 = LSTM(LSTM_UNITS, return_sequences=True, return_state=True, dropout=0.3, recurrent_dropout=0.3) # first LSTM layer for decoder\n",
    "decoder_outputs_1, state_dh1, state_dc1 = decoder_lstm_1(decoder_concat_input)\n",
    "\n",
    "# Second LSTM layer for decoder\n",
    "decoder_lstm_2 = LSTM(LSTM_UNITS, return_sequences=True, return_state=True, dropout=0.3, recurrent_dropout=0.3) # second LSTM layer for decoder\n",
    "decoder_outputs_2, _, _ = decoder_lstm_2(decoder_outputs_1)\n",
    "\n",
    "decoder_dense = Dense(VOCAB_SIZE, activation='softmax') # dense output layer with softmax activation function\n",
    "decoder_outputs = decoder_dense(decoder_outputs_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda4a90c-dd9b-408e-bc27-3e866bd7aaae",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">\n",
    "    The above configuration has two main parts: an encoder and a decoder. The encoder takes in the sequences and uses two LSTM layers to understand the content, producing a context for the decoder. The decoder then uses attention to focus on certain parts of the encoder's output, passes it through two more LSTM layers, and finally uses a dense layer to guess the next word in the answer.\n",
    "</p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34105340-34f4-4dd5-895b-6f0b807339e6",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px; margin-top: 10px; font-weight: bold\">Model Compilation</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e428d24f-6c26-4d3c-95e7-1aa49f262f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, None, 100)    13555900    ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, None, 128),  117248      ['embedding[0][0]']              \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, None, 128),  131584      ['lstm[0][0]']                   \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " bahdanau_attention (BahdanauAt  ((None, None, 128),  33153      ['lstm_1[0][1]',                 \n",
      " tention)                        (None, None, 1))                 'lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, None, 100)    13555900    ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, None, 228)    0           ['bahdanau_attention[0][0]',     \n",
      "                                                                  'embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  [(None, None, 128),  182784      ['concatenate[0][0]']            \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  [(None, None, 128),  131584      ['lstm_2[0][0]']                 \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, None, 135559  17487111    ['lstm_3[0][0]']                 \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 45,195,264\n",
      "Trainable params: 45,195,264\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs) # model function from keras to define the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) # model compilation after its definition\n",
    "model.summary() # it will gives the printout of the model's architecture which has been set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b72e07f5-50b9-4300-be07-ae5012952745",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping( # callback function in kersa to halt the training process when there is no improvement in the model\n",
    "    monitor='val_loss',       # monitor validation loss\n",
    "    patience=4,             # number of epochs with no improvement after which training will be stopped.\n",
    "    verbose=1,               # To display logs\n",
    "    restore_best_weights=True # best weights (lowest validation loss) will be restored into the model.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86681c2e-03fe-483b-93bb-c5f4586b7283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-04 16:41:17.511852: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-09-04 16:41:18.039074: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f1eac23f9b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-09-04 16:41:18.039088: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2023-09-04 16:41:18.049829: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-09-04 16:41:18.201105: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12355/12355 [==============================] - 13179s 1s/step - loss: 3.1657 - accuracy: 0.5090 - val_loss: 0.6993 - val_accuracy: 0.9165\n",
      "Epoch 2/6\n",
      "12355/12355 [==============================] - 13122s 1s/step - loss: 0.4625 - accuracy: 0.9425 - val_loss: 0.2797 - val_accuracy: 0.9724\n",
      "Epoch 3/6\n",
      "12355/12355 [==============================] - 13128s 1s/step - loss: 0.2374 - accuracy: 0.9727 - val_loss: 0.2044 - val_accuracy: 0.9809\n",
      "Epoch 4/6\n",
      "12355/12355 [==============================] - 13170s 1s/step - loss: 0.1705 - accuracy: 0.9803 - val_loss: 0.1789 - val_accuracy: 0.9839\n",
      "Epoch 5/6\n",
      "12355/12355 [==============================] - 13152s 1s/step - loss: 0.1298 - accuracy: 0.9843 - val_loss: 0.1604 - val_accuracy: 0.9861\n",
      "Epoch 6/6\n",
      "12355/12355 [==============================] - 13157s 1s/step - loss: 0.0967 - accuracy: 0.9867 - val_loss: 0.1566 - val_accuracy: 0.9870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f266786bd50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit function to train the model on the training data, evaluates it on the validation data after each epoch and use the earlystopping function which has been defined before to halt the training process if the model is not improving \n",
    "model.fit([questions_train, answers_train], np.expand_dims(answers_train, -1),\n",
    "                    batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "                    validation_data=([questions_val, answers_val], np.expand_dims(answers_val, -1)),\n",
    "                    callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b946927-5317-40c3-b749-9d40568ac7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('medical_encoder_LSTM.h5') # save the trained model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe08c41-f4f2-4b2e-857f-9086a3efd888",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a689f5-7ad3-4ef8-8864-3fa5a29a5563",
   "metadata": {},
   "source": [
    "<p ><center><u style=\"font-size: 28px; margin-top: 10px; font-weight: bold\">Model Evaluation</u></center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ab1cb8-8fb9-44eb-a9d7-4cca1e21cc27",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">\n",
    "Model evaluation is an important part of creating machine learning models. It's about testing how good the model is using data it hasn't seen during training, usually called test or validation data. We do this to see if the model's answers are right and understand any mistakes it might make.\n",
    "</p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1024f435-c5b7-4391-8622-629eb788f330",
   "metadata": {},
   "source": [
    "<div style=\"width:100%;height:1px; background-color:black\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4e10429-1531-4e6e-8906-2ea2f55fdb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = load_model('medical_encoder_LSTM.h5', custom_objects={'BahdanauAttention': BahdanauAttention}) # load the trained model from the directory with the attention layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90886978-9c75-403a-91c4-42458684d0b2",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px; margin-top: 10px; font-weight: bold\">Accuracy and Loss</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f30daad-2c6c-4294-9ea6-ff85c4585b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-06 02:11:39.992760: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "387/387 [==============================] - 137s 352ms/step - loss: 0.0744 - accuracy: 0.9905\n",
      "Accuracy: 0.9905073642730713\n",
      "Loss: 0.07438204437494278\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = loaded_model.evaluate([questions_val, answers_val], np.expand_dims(answers_val, -1)) #evaluate the model using validation dataset\n",
    "print('Accuracy:', accuracy)\n",
    "print('Loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb8db4e-470e-4db9-9871-b0bfbe39df50",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">\n",
    "    In a medical chatbot using doctor-patient dialogues, just checking accuracy might not be enough. Medical conversations can have many right answers, and missing an important detail can be risky. Also, if the bot often gets rare diseases wrong but common ones right, its accuracy can still look high. So, other ways of checking how good the bot is, like seeing how often it gets rare cases right, might be better.\n",
    "</p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b22563-854e-499f-9b74-60a9ac5fc2ea",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">\n",
    "    In the context of medical chatbot having doctor-patient dialogues, perplexity and ROUGE scores can be helpful metrics. They evaluate how closely the bot's answers correspond to those offered by experts. While ROUGE examines the overlap of n-grams while taking precision and recall into account, BLEU focuses on word and phrase matching. The capacity of the chatbot to produce responses that correspond with those that a medical practitioner could say can be inferred from both scores.  \n",
    "</p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16baccec-2ce5-4a0d-beab-8bcec8fef387",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px; margin-top: 10px; font-weight: bold\">Perplexity</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704ee341-43fa-49d1-b11c-6f868989655a",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">\n",
    "    Perplexity checks how good a model is at guessing the next word. For medical chatbots, a lower value means the bot can chat more smoothly and make sense.\n",
    "</p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e62e745b-e226-4905-bc62-2d1c58221cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 1.0529099420153887\n"
     ]
    }
   ],
   "source": [
    "perplexity = 2**(loss) # calculate perplexity\n",
    "print('Perplexity:', perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef72d355-9818-4299-8917-3227fb161317",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px; margin-top: 10px; font-weight: bold\">ROUGE score</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85d4db6-5a9d-4fa3-ae3f-27c813614218",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">\n",
    "    The ROUGE score looks at how much the predicted text matches the reference text using different measures like precision, recall, and F1-score. It's particularly useful for tasks like summarization to see how much key information the model includes in its output. In the context of medical chatbot, ROUGE can help determine how closely the generated response matches a desired or reference answer, indicating the system's ability to provide accurate and relevant information.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e041c7a7-fde8-4613-a248-7a6e2f8aa06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_val_subset = questions_val[:30]\n",
    "answers_val_subset = answers_val[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11064a8d-1dc5-4a59-aa6a-a58727b4a559",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 6s 194ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-06 02:14:02.504344: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 4164372480 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "predictions = loaded_model.predict([questions_val_subset, answers_val_subset], batch_size=1) # generate predictions from the model using validation data\n",
    "predicted_texts = [tokenizer.sequences_to_texts([pred]) for pred in predictions.argmax(axis=-1)] # converting predictions to text\n",
    "references = [tokenizer.sequences_to_texts([ans]) for ans in answers_val_subset] # convert answers to text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "067aa791-3d07-46fc-80b9-d3913400321f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE score: {'rouge-1': {'r': 0.9851534188882684, 'p': 0.967791314984092, 'f': 0.9763772069305389}, 'rouge-2': {'r': 0.9759466361049649, 'p': 0.9464615367846199, 'f': 0.9609346073301777}, 'rouge-l': {'r': 0.9851534188882684, 'p': 0.967791314984092, 'f': 0.9763772069305389}}\n"
     ]
    }
   ],
   "source": [
    "predicted_texts = [\" \".join(text) if isinstance(text, list) else text for text in predicted_texts] #predicted text from list of words to a single string for each prediction\n",
    "references = [\" \".join(text) if isinstance(text, list) else text for text in references] # convert reference texts (actual answers) from list of words to a single string \n",
    "rouge = Rouge() # initialize the Rouge method\n",
    "scores = rouge.get_scores(predicted_texts, references, avg=True) # calculate ROUGE scores by comparing predicted texts to reference texts\n",
    "print('ROUGE score:', scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ec5f3f-0f5d-434b-a97f-1aa224a261ec",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 23px; margin-top: 10px; font-weight: bold\"><u>Saving Results to the dataframe</u></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333afe59-aaaa-4f66-9861-76a38c199a4f",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 20px\">\n",
    "  I am saving the results in the dataframe one by one of each model so i can compare the results in the separate python file (medical_chatbot_eval_metrics.ipynb).\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14cb001f-4b19-472e-b79a-f25afc81a45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>loss</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>rouge-1_r</th>\n",
       "      <th>rouge-1_p</th>\n",
       "      <th>rouge-1_f</th>\n",
       "      <th>rouge-2_r</th>\n",
       "      <th>rouge-2_p</th>\n",
       "      <th>rouge-2_f</th>\n",
       "      <th>rouge-l_r</th>\n",
       "      <th>rouge-l_p</th>\n",
       "      <th>rouge-l_f</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Encoder-Decoder LSTM</td>\n",
       "      <td>0.074382</td>\n",
       "      <td>1.05291</td>\n",
       "      <td>0.990507</td>\n",
       "      <td>0.985153</td>\n",
       "      <td>0.967791</td>\n",
       "      <td>0.976377</td>\n",
       "      <td>0.975947</td>\n",
       "      <td>0.946462</td>\n",
       "      <td>0.960935</td>\n",
       "      <td>0.985153</td>\n",
       "      <td>0.967791</td>\n",
       "      <td>0.976377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             model_name      loss  perplexity  accuracy  rouge-1_r  rouge-1_p  \\\n",
       "0  Encoder-Decoder LSTM  0.074382     1.05291  0.990507   0.985153   0.967791   \n",
       "\n",
       "   rouge-1_f  rouge-2_r  rouge-2_p  rouge-2_f  rouge-l_r  rouge-l_p  rouge-l_f  \n",
       "0   0.976377   0.975947   0.946462   0.960935   0.985153   0.967791   0.976377  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "eval_metrics_results = {\n",
    "    'model_name': 'Encoder-Decoder LSTM',\n",
    "    'loss': loss,\n",
    "    'perplexity': perplexity,\n",
    "    'accuracy': accuracy,\n",
    "    'rouge-1_r': scores['rouge-1']['r'],\n",
    "    'rouge-1_p': scores['rouge-1']['p'],\n",
    "    'rouge-1_f': scores['rouge-1']['f'],\n",
    "    'rouge-2_r': scores['rouge-2']['r'],\n",
    "    'rouge-2_p': scores['rouge-2']['p'],\n",
    "    'rouge-2_f': scores['rouge-2']['f'],\n",
    "    'rouge-l_r': scores['rouge-l']['r'],\n",
    "    'rouge-l_p': scores['rouge-l']['p'],\n",
    "    'rouge-l_f': scores['rouge-l']['f']\n",
    "}\n",
    "\n",
    "eval_metrics_results_dataframe = pd.DataFrame([eval_metrics_results])\n",
    "\n",
    "eval_metrics_results_dataframe.to_csv('eval_metrics_results_dataframe.csv', index=False) # save the dataframe\n",
    "\n",
    "eval_metrics_results_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73fa89f-c7cd-4070-8544-a881887c0b17",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 23px; margin-top: 10px; font-weight: bold\"><u>Answer to user queries by using the model</u></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe3e4060-5162-41da-ae1b-e39a9fbe6524",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your medical query:  what is flu?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 821ms/step\n",
      "Medical assitant flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a virus flu is a viru\n"
     ]
    }
   ],
   "source": [
    "user_query = input(\"Enter your medical query: \")\n",
    "response = answer_query(user_query, loaded_model, tokenizer, 256)\n",
    "print('Medical assitant', response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e770a472-2044-4cc3-8ae0-2399bf89c873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
